{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import enum\n",
    "\n",
    "from spear.labeling import labeling_function, LFSet, ABSTAIN, preprocessor\n",
    "\n",
    "from examples.TREC.preprocessor import convert_to_lower\n",
    "\n",
    "class ClassLabels(enum.Enum):\n",
    "    DESCRIPTION     = 0\n",
    "    ENTITY          = 1\n",
    "    HUMAN           = 2\n",
    "    ABBREVIATION    = 3\n",
    "    LOCATION        = 4\n",
    "    NUMERIC         = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rules(file_name='rules.txt'):\n",
    "    rules = LFSet(\"TREC_LFS\")\n",
    "    \n",
    "    with open(file_name, 'r', encoding='latin1') as f:\n",
    "        i = 0\n",
    "        for line in f:\n",
    "            list_in = line.strip().split(\"\\t\")\n",
    "            label = ClassLabels[list_in[0]]\n",
    "            pattern = list_in[1]\n",
    "            rule_name = \"rule\"+str(i)\n",
    "            \n",
    "            @labeling_function(name=rule_name,resources=dict(pattern=pattern),pre=[convert_to_lower],label=label)\n",
    "            def f(x,**kwargs):\n",
    "                result = re.findall(kwargs[\"pattern\"], x)\n",
    "                if result:\n",
    "                    return label\n",
    "                else:\n",
    "                    return ABSTAIN\n",
    "\n",
    "            rules.add_lf(f)\n",
    "            i = i+1\n",
    "    return rules\n",
    "\n",
    "rules = load_rules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import preprocessor\n",
    "\n",
    "@preprocessor()\n",
    "def convert_to_lower(x):\n",
    "    return x.lower().strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.labeling import PreLabels\n",
    "from notebooks.TREC.utils import load_data_to_numpy\n",
    "\n",
    "X_V, X_feats_V, Y_V = load_data_to_numpy(file_name='valid.txt')\n",
    "X_T, X_feats_T, Y_T = load_data_to_numpy(file_name='test.txt')\n",
    "X, X_feats, Y = load_data_to_numpy(file_name='train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_L, X_feats_L, Y_L, X_U, X_feats_U, Y_U = X[:100] , X_feats[:100], Y[:100], X[100:], X_feats[100:], Y[100:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((100,), (5352,), (500,), (500,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_L.shape, X_U.shape, X_V.shape, X_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X, X_feats, Y = load_data_to_numpy()\n",
    "# Y = np.array([ClassLabels[x].value for x in Y])\n",
    "\n",
    "# trec_noisy_labels = PreLabels(name=\"trec\",\n",
    "#                                data=X,\n",
    "#                                gold_labels=Y,\n",
    "#                                data_feats=X_feats,\n",
    "#                                rules=rules,\n",
    "#                                labels_enum=ClassLabels,\n",
    "#                                num_classes=6)\n",
    "# L,S = trec_noisy_labels.get_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from helper.utils import load_data_to_numpy, get_various_data\n",
    "\n",
    "# X_V = \n",
    "\n",
    "# X, X_feats, Y = load_data_to_numpy()\n",
    "\n",
    "# validation_size = 152\n",
    "# test_size = 500\n",
    "# L_size = 100\n",
    "# U_size = 4700\n",
    "# n_lfs = len(rules.get_lfs())\n",
    "\n",
    "# X_V, Y_V, X_feats_V,_, X_T, Y_T, X_feats_T,_, X_L, Y_L, X_feats_L,_, X_U, X_feats_U,_ = get_various_data(X, Y,\\\n",
    "#     X_feats, n_lfs, validation_size, test_size, L_size, U_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_json = 'data_pipeline/trec_json.json'\n",
    "V_path_pkl = 'data_pipeline/trec_pickle_V.pkl' #validation data - have true labels\n",
    "T_path_pkl = 'data_pipeline/trec_pickle_T.pkl' #test data - have true labels\n",
    "L_path_pkl = 'data_pipeline/trec_pickle_L.pkl' #Labeled data - have true labels\n",
    "U_path_pkl = 'data_pipeline/trec_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "\n",
    "log_path_cage_1 = 'log/trec_cage_log_1.txt' #cage is an algorithm, can be found below\n",
    "log_path_jl_1 = 'log/trec_jl_log_1.txt' #jl is an algorithm, can be found below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [00:00<00:00, 1622.76it/s]\n",
      "100%|██████████| 500/500 [00:00<00:00, 2203.44it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 1643.51it/s]\n",
      "100%|██████████| 5352/5352 [00:03<00:00, 1739.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from spear.labeling import PreLabels\n",
    "\n",
    "trec_noisy_labels = PreLabels(name=\"trec\",\n",
    "                               data=X_V,\n",
    "                               gold_labels=Y_V,\n",
    "                               data_feats=X_feats_V,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=6)\n",
    "trec_noisy_labels.generate_pickle(V_path_pkl)\n",
    "trec_noisy_labels.generate_json(path_json) #generating json files once is enough\n",
    "\n",
    "trec_noisy_labels = PreLabels(name=\"trec\",\n",
    "                               data=X_T,\n",
    "                               gold_labels=Y_T,\n",
    "                               data_feats=X_feats_T,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=6)\n",
    "trec_noisy_labels.generate_pickle(T_path_pkl)\n",
    "\n",
    "trec_noisy_labels = PreLabels(name=\"trec\",\n",
    "                               data=X_L,\n",
    "                               gold_labels=Y_L,\n",
    "                               data_feats=X_feats_L,\n",
    "                               rules=rules,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=6)\n",
    "trec_noisy_labels.generate_pickle(L_path_pkl)\n",
    "\n",
    "trec_noisy_labels = PreLabels(name=\"trec\",\n",
    "                               data=X_U,\n",
    "                               rules=rules,\n",
    "                               data_feats=X_feats_U,\n",
    "                               labels_enum=ClassLabels,\n",
    "                               num_classes=6)\n",
    "trec_noisy_labels.generate_pickle(U_path_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in data list:  10\n",
      "Shape of feature matrix:  (5352, 1024)\n",
      "Shape of labels matrix:  (5352, 68)\n",
      "Shape of continuous scores matrix :  (5352, 68)\n",
      "Total number of classes:  6\n",
      "Classes dictionary in json file(modified to have integer keys):  {0: 'DESCRIPTION', 1: 'ENTITY', 2: 'HUMAN', 3: 'ABBREVIATION', 4: 'LOCATION', 5: 'NUMERIC'}\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_data, get_classes\n",
    "\n",
    "data_U = get_data(path = U_path_pkl, check_shapes=True)\n",
    "#check_shapes being True(above), asserts for relative shapes of arrays in pickle file\n",
    "print(\"Number of elements in data list: \", len(data_U))\n",
    "print(\"Shape of feature matrix: \", data_U[0].shape)\n",
    "print(\"Shape of labels matrix: \", data_U[1].shape)\n",
    "print(\"Shape of continuous scores matrix : \", data_U[6].shape)\n",
    "print(\"Total number of classes: \", data_U[9])\n",
    "\n",
    "classes = get_classes(path = path_json)\n",
    "print(\"Classes dictionary in json file(modified to have integer keys): \", classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from spear.JL import JL\n",
    "\n",
    "# n_features = 1024\n",
    "# n_hidden = 512\n",
    "# feature_model = 'lr'\n",
    "\n",
    "# jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, n_hidden = n_hidden, \\\n",
    "#         feature_model = feature_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from spear.JL import JL\n",
    "loss_func_mask = [1,1,1,1,1,1,1]\n",
    "'''\n",
    "One can keep 0s in places where he don't want the specific loss function to be part\n",
    "the final loss function used in training. Refer documentation(spear.JL.core.JL) to understand\n",
    "the which index of loss_func_mask refers to what loss function.\n",
    "Note: the loss_func_mask may not be the optimal mask for sms dataset.\n",
    "'''\n",
    "batch_size = 150\n",
    "lr_fm = 0.0005\n",
    "lr_gm = 0.01\n",
    "use_accuracy_score = False\n",
    "feature_model = 'nn'\n",
    "n_features = 1024\n",
    "n_hidden = 512\n",
    "n_lfs = len(rules.get_lfs())\n",
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, n_hidden = n_hidden, \\\n",
    "        feature_model = feature_model)\n",
    "\n",
    "probs_fm, probs_gm = jl.fit_and_predict_proba(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = True, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'macro')\n",
    "\n",
    "labels = np.argmax(probs_fm, 1)\n",
    "print(\"probs_fm shape: \", probs_fm.shape)\n",
    "print(\"probs_gm shape: \", probs_gm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
