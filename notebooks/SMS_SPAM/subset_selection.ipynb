{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12fde778",
   "metadata": {},
   "source": [
    "# ***Subset selection:***\n",
    "This notebook aims at demonstrating the use cases for the functions in spear library for subset selection. Subset selection is selecting a small subset of unlabeled data(or the data labeled by LFs, in case of supervised subset selection) so that it can be labeled and use that small labeled data(the L dataset) for effective training of <b>JL algorithm</b>(Cage algorithm doesn't need labeled data). Finding the best subset makes best use of the labeling efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f499ea92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d757bf",
   "metadata": {},
   "source": [
    "### **Random subset selection**\n",
    "Here we select a random subset of instances to label. We need number of instances available and number of instances we intend to label to get a sorted numpy array of indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "356ba6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "indices selected by rand_subset:  [ 2  3  8 12 18]\n",
      "return type of rand_subset:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from spear.JL import rand_subset\n",
    "\n",
    "indices = rand_subset(n_all = 20, n_instances = 5) #select 5 instances from a total of 20 instances\n",
    "print(\"indices selected by rand_subset: \", indices)\n",
    "print(\"return type of rand_subset: \", type(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60cecb70",
   "metadata": {},
   "source": [
    "### **Unsupervised subset selection**\n",
    "Here we select a unsupervised subset(for more on this, please refer [here](https://arxiv.org/abs/2008.09887) ) of instances to label. We need feature matrix(of shape (num_instaces, num_features)) and number of instances we intend to label and we get a sorted numpy array of indices. For any other arguments to unsup_subset(or to sup_subset_indices or sup_subset_save_files) please refer documentation.\n",
    "<p>For this let's first get some data(feature matrix), say from sms_pickle_U.pkl(in data_pipeline folder). For more on this pickle file, please refer the other notebook named sms_cage_jl.ipynb</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "449ec58b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_U shape:  (4500, 1024)\n",
      "x_U type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from spear.utils import get_data, get_classes\n",
    "\n",
    "U_path_pkl = 'data_pipeline/JL/sms_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "data_U = get_data(U_path_pkl, check_shapes=True)\n",
    "x_U = data_U[0] #the feature matrix\n",
    "print(\"x_U shape: \", x_U.shape)\n",
    "print(\"x_U type: \", type(x_U))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55c16aa",
   "metadata": {},
   "source": [
    "Now that we have feature matrix, let's select the indices to label from it. After labeling(through a trustable means) those instances, whose indices(index with respect to feature matrix) are given by the following function, one can pass them as gold_labels to the PreLabels class in the process for labeling the subset-selected data and forming a pickle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6972ed8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 indices given by unsup_subset:  [  94  150  302  783 1255 1524 1560 2098 2270 2485]\n",
      "return type of unsup_subset:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from spear.JL import unsup_subset\n",
    "\n",
    "indices = unsup_subset(x_train = x_U, n_unsup = 20)\n",
    "print(\"first 10 indices given by unsup_subset: \", indices[:10])\n",
    "print(\"return type of unsup_subset: \", type(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b7f408",
   "metadata": {},
   "source": [
    "### **Supervised subset selection**\n",
    "Here we select a supervised subset(for more on this, please refer [here](https://arxiv.org/abs/2008.09887) ) of instances to label. We need \n",
    "* path to json file having information about classes\n",
    "* path to pickle file generated by feature matrix after labeling using LFs\n",
    "* number of instances we intend to label\n",
    "\n",
    "<p>we get a sorted numpy array of indices.</p>\n",
    "<p>For this let's use sms_json.json, sms_pickle_U.pkl(in data_pipeline folder). For more on this json/pickle file, please refer the other notebook named sms_cage_jl.ipynb</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8659db63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 indices given by sup_subset:  [ 26  38  59 151 185 247 255 315 336 407]\n",
      "return type of sup_subset:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from spear.JL import sup_subset_indices\n",
    "\n",
    "U_path_pkl = 'data_pipeline/JL/sms_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "path_json = 'data_pipeline/JL/sms_json.json'\n",
    "indices = sup_subset_indices(path_json = path_json, path_pkl = U_path_pkl, n_sup = 100, qc = 0.85)\n",
    "\n",
    "print(\"first 10 indices given by sup_subset: \", indices[:10])\n",
    "print(\"return type of sup_subset: \", type(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e8d97",
   "metadata": {},
   "source": [
    "Instead of just getting indices to already labeled data(stored in pickle format, using LFs), we also provide the following utility to split the input pickle file and save two pickle files on the basis of subset selection. Make sure that path_save_L and path_save_U are <b>EMPTY</b> pickle file. You can still get the return value of subset-selected indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dd0353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 indices given by sup_subset:  [ 26  38  59 151 185 247 255 315 336 407]\n",
      "return type of sup_subset:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from spear.JL import sup_subset_save_files\n",
    "\n",
    "U_path_pkl = 'data_pipeline/JL/sms_pickle_U.pkl' #unlabelled data - don't have true labels\n",
    "path_json = 'data_pipeline/JL/sms_json.json'\n",
    "path_save_L = 'data_pipeline/JL/sup_subset_L.pkl'\n",
    "path_save_U = 'data_pipeline/JL/sup_subset_U.pkl'\n",
    "\n",
    "indices = sup_subset_save_files(path_json = path_json, path_pkl = U_path_pkl, path_save_L = path_save_L, \\\n",
    "                             path_save_U = path_save_U, n_sup = 100, qc = 0.85)\n",
    "\n",
    "print(\"first 10 indices given by sup_subset: \", indices[:10])\n",
    "print(\"return type of sup_subset: \", type(indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd3800e",
   "metadata": {},
   "source": [
    "### **Inserting true labels into pickle files**\n",
    "Now after doing supervised subset selection, say we get two pickle files path_save_L and path_save_U. Now say you labeled the instances of path_save_L and want to insert them into pickle file. So here, instead of going over the process of generating pickle through PreLabels again, you can use the following function to create a new pickle file, which now contain true labels, using path_save_L pickle file. There is no return value to this function. Make sure that path_save, the pickle file path that is to be formed with the data in path_save_L file and true labels, is <b>EMPTY</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d68700e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.JL import insert_true_labels\n",
    "\n",
    "path_save_L = 'data_pipeline/JL/sup_subset_L.pkl'\n",
    "path_save_labeled = 'data_pipeline/JL/sup_subset_labeled_L.pkl'\n",
    "labels = np.random.randint(0,2,[100, 1])\n",
    "'''\n",
    "Above is just a random association of labels used for demo. In real time user has to label the instances in\n",
    "path_save_L with a trustable means and use it here.\n",
    "\n",
    "Note that the shape of labels is (num_instances, 1) and just for reference, feature_matrix(the first element\n",
    "in pickle file) in path_save_L has shape (num_instances, num_features).\n",
    "'''\n",
    "insert_true_labels(path = path_save_L, path_save = path_save_labeled, labels = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccbd2f3f",
   "metadata": {},
   "source": [
    "A similar function as insert_true_labels called replace_in_pkl is also made available to make changes to pickle file. replace_in_pkl usage is demonstrated below. Note that replace_in_pkl doesn't edit the pickle file, instead creates a new pickle file. Make sure that path_save, the pickle file path that is to be formed with the data in path file and a new numpy array, is <b>EMPTY</b>. There is no return value for this function too.\n",
    "<p>It is highly advised to use insert_true_labels function for the purpose of inserting the labels since it does some other necessary changes.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fc5e52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.JL import replace_in_pkl\n",
    "\n",
    "path_labeled = 'data_pipeline/JL/sup_subset_labeled_L.pkl' # aka path_save_labeled\n",
    "path_save_altered = 'data_pipeline/JL/sup_subset_altered_L.pkl'\n",
    "np_array = np.random.randint(0,2,[100, 1]) #we are just replacing the labels we inserted before\n",
    "index = 3 \n",
    "'''\n",
    "index refers to the element we intend to replace. Refer documentaion(specifically \n",
    "spear.utils.data_editor.get_data) to understand which numpy array an index value\n",
    "maps to(order the contents of pickle file from 0 to 8). Index should be in range [0,8].\n",
    "'''\n",
    "\n",
    "replace_in_pkl(path = path_labeled, path_save = path_save_altered, np_array = np_array, index = index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee87f1",
   "metadata": {},
   "source": [
    "### **Demonstrating the use of labeled subset-selected data**\n",
    "Now that we have our subset(labeled) in path_save_labeled, lets see a use case by calling a member function of JL class using path_save_labeled as our path to L data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c82c3681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "early stopping at epoch: 21\tbest_epoch: 10\n",
      "score used: f1_score\n",
      "best_gm_val_score:0.6153846153846153\tbest_fm_val_score:0.6666666666666667\n",
      "best_gm_test_score:0.5714285714285714\tbest_fm_test_score:0.6776859504132232\n",
      "best_gm_test_precision:0.42718446601941745\tbest_fm_test_precision:0.5857142857142857\n",
      "best_gm_test_recall:0.8627450980392157\tbest_fm_test_recall:0.803921568627451\n",
      "probs_fm shape:  (4400, 2)\n",
      "probs_gm shape:  (4400, 2)\n"
     ]
    }
   ],
   "source": [
    "from spear.JL import JL\n",
    "\n",
    "n_lfs = 16\n",
    "n_features = 1024\n",
    "n_hidden = 512\n",
    "feature_model = 'nn'\n",
    "path_json = 'data_pipeline/JL/sms_json.json'\n",
    "\n",
    "jl = JL(path_json = path_json, n_lfs = n_lfs, n_features = n_features, n_hidden = n_hidden, \\\n",
    "        feature_model = feature_model)\n",
    "\n",
    "L_path_pkl = path_save_labeled #Labeled data - have true labels\n",
    "U_path_pkl = path_save_U #unlabelled data - don't have true labels\n",
    "V_path_pkl = 'data_pipeline/JL/sms_pickle_V.pkl' #validation data - have true labels\n",
    "T_path_pkl = 'data_pipeline/JL/sms_pickle_T.pkl' #test data - have true labels\n",
    "log_path_jl_1 = 'log/JL/jl_log_1.txt'\n",
    "loss_func_mask = [1,1,1,1,1,1,1] \n",
    "batch_size = 150\n",
    "lr_fm = 0.0005\n",
    "lr_gm = 0.01\n",
    "use_accuracy_score = False\n",
    "\n",
    "probs_fm, probs_gm = jl.fit_and_predict_proba(path_L = L_path_pkl, path_U = U_path_pkl, path_V = V_path_pkl, \\\n",
    "        path_T = T_path_pkl, loss_func_mask = loss_func_mask, batch_size = batch_size, lr_fm = lr_fm, lr_gm = \\\n",
    "    lr_gm, use_accuracy_score = use_accuracy_score, path_log = log_path_jl_1, return_gm = True, n_epochs = \\\n",
    "    100, start_len = 7,stop_len = 10, is_qt = True, is_qc = True, qt = 0.9, qc = 0.85, metric_avg = 'binary')\n",
    "\n",
    "labels = np.argmax(probs_fm, 1)\n",
    "print(\"probs_fm shape: \", probs_fm.shape)\n",
    "print(\"probs_gm shape: \", probs_gm.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
