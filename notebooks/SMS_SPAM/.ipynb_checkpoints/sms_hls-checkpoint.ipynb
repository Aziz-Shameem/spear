{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd61e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944fe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_data_feeders\n",
    "import collections\n",
    "\n",
    "f_d = 'f_d'\n",
    "f_d_U = 'f_d_U'\n",
    "test_w = 'test_w'\n",
    "\n",
    "train_modes = [f_d, f_d_U]\n",
    "\n",
    "F_d_U_Data = collections.namedtuple('GMMDataF_d_U', 'x l m L d r')\n",
    "F_d_Data = collections.namedtuple('GMMDataF_d', 'x labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2758a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/parth/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from spear.Implyloss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016a1991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint file does not exist\n",
      "INFO:tensorflow:best.ckpt-51 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-51\n",
      "Restoring best checkpoint from path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:best.ckpt-52 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-52\n",
      "Restoring best checkpoint from path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:best.ckpt-53 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-53\n",
      "INFO:tensorflow:best.ckpt-54 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-54\n",
      "Restoring best checkpoint from path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-53\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-53\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.354182/best.ckpt-12 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.354182/best.ckpt-13 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.354182/best.ckpt-14 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Best ckpt path:  /tmp/best_ckpt_0.354182/best.ckpt-13\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.354182/best.ckpt-13\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.354182/best.ckpt-15 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Best ckpt path:  /tmp/best_ckpt_0.354182/best.ckpt-13\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.354182/best.ckpt-13\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.354182/best.ckpt-16 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Best ckpt path:  /tmp/best_ckpt_0.354182/best.ckpt-16\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.354182/best.ckpt-16\n",
      "Best ckpt path:  /tmp/best_ckpt_0.354182/best.ckpt-15\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.354182/best.ckpt-15\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints/hls-model\n",
      "Restoring checkpoint from path:  /tmp/checkpoints/hls-model\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints/hls-model\n",
      "Restoring checkpoint from path:  /tmp/checkpoints/hls-model\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints/hls-model\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.741876/hls-model-11\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.741876/hls-model-11\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.741876/hls-model-11\n",
      "WARNING:tensorflow:From /home/parth/.local/lib/python3.8/site-packages/tensorflow/python/training/saver.py:968: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.741876/hls-model-5\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.741876/hls-model-5\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.741876/hls-model-5\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.582258/hls-model-11\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.582258/hls-model-11\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.582258/hls-model-11\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.582258/hls-model-5\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.582258/hls-model-5\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.582258/hls-model-5\n"
     ]
    }
   ],
   "source": [
    "#my_checkpoints\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# from my_checkmate import BestCheckpointSaver, get_best_checkpoint\n",
    "# from my_data_types import train_modes\n",
    "checkpoint_dir = \"./checkpoint\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "\tos.makedirs(checkpoint_dir)\n",
    "\n",
    "num_checkpoints = 1 # Number of checkpoints to keep around\n",
    "\n",
    "# Keeps only the most recently saved checkpoint\n",
    "#\n",
    "# max_to_keep is deliberatly set to 1 in order to provide for the case when more recent checkpoint\n",
    "# has a smaller global_step. tf.train.Saver() orders by global_step.\n",
    "\n",
    "class MRUCheckpoint():\n",
    "\tdef __init__(self, path, session, variables):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tInitializes the class variables\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tpath - file path\n",
    "\t\tsession \n",
    "\t\tvariables\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tself.ckpt_path = path\n",
    "\t\tself.ckpt_file = os.path.join(path, 'checkpoint')\n",
    "\t\tself.checkpoint_prefix = os.path.join(self.ckpt_path, 'hls-model')\n",
    "\t\tself.sess = session\n",
    "\t\t# max_to_keep\n",
    "\t\tself.saver = tf.train.Saver(variables, max_to_keep=1)\n",
    "\t\t# self.saver = tf.train.Saver()\n",
    "\n",
    "\tdef save(self, global_step=None):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tsaves the obtained checkpoint\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tglobal step (Default - none)\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tpath = self.saver.save(self.sess, self.checkpoint_prefix, global_step)\n",
    "\t\tprint('Saved MRU checkpoint to path: ', path)\n",
    "\t\t\n",
    "\tdef restore(self):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tRestores the last checkpoint\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tlast_checkpoint = tf.train.latest_checkpoint(self.ckpt_path, 'checkpoint')\n",
    "\t\t#if self.saver.last_checkpoints:\n",
    "\t\t#    last_checkpoint = self.saver.last_checkpoints[0]\n",
    "\t\t#    print('All saved checkpoints: ', self.saver.last_checkpoints)\n",
    "\t\t#else:\n",
    "\t\tif not last_checkpoint:\n",
    "\t\t\tlast_checkpoint = self.checkpoint_prefix\n",
    "\n",
    "\t\tprint('Restoring checkpoint from path: ', last_checkpoint)\n",
    "\t\tself.saver.restore(self.sess, last_checkpoint)\n",
    "\n",
    "\tdef restore_if_checkpoint_exists(self):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tchecks if there exists any checkpoint for the file \n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\n",
    "\t\tOutput:\n",
    "\t\tBoolean (True or False)\n",
    "\t\t'''\n",
    "\t\tif os.path.exists(self.ckpt_file):\n",
    "\t\t\tself.restore()\n",
    "\t\t\treturn True\n",
    "\t\treturn False\n",
    "\n",
    "def test_mru_checkpoints(num_to_keep):\n",
    "\t'''\n",
    "\tFunc Desc:\n",
    "\tRuns different sessions while changing the checkpoint number that is currently being worked with and tests the same\n",
    "\t\n",
    "\tInput:\n",
    "\tnum_to_keep(int) - a limit on the size of the global step for checkpoint traversal\n",
    "\n",
    "\tOutput:\n",
    "\n",
    "\t'''\n",
    "\tglobal_step = tf.get_variable(name='mru_global_step_%d' % num_to_keep, initializer=10, dtype=tf.int32)\n",
    "\tinc = tf.assign_add(global_step, 1)\n",
    "\tsess = tf.Session()\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\tassert sess.run(global_step) == 10\n",
    "\n",
    "\tsess.run(inc)\n",
    "\tassert sess.run(global_step) == 11\n",
    "\tckpt_path = '/tmp/checkpoints_%.6f' % np.random.rand()\n",
    "\tckpt = MRUCheckpoint(ckpt_path, sess, tf.global_variables())\n",
    "\tckpt.save(global_step)\n",
    "\n",
    "\tsess.run(inc)\n",
    "\tassert sess.run(global_step) == 12\n",
    "\n",
    "\tckpt.restore_if_checkpoint_exists()\n",
    "\tassert sess.run(global_step) == 11\n",
    "\n",
    "\tassgn_op = tf.assign(global_step, 5)\n",
    "\tsess.run(assgn_op)\n",
    "\tassert sess.run(global_step) == 5\n",
    "\tckpt.save(global_step)\n",
    "\n",
    "\tsess.run(inc)\n",
    "\tsess.run(inc)\n",
    "\tassert sess.run(global_step) == 7\n",
    "\n",
    "\tckpt.restore_if_checkpoint_exists()\n",
    "\tassert sess.run(global_step) == 5\n",
    "\n",
    "def test_checkpoint():\n",
    "\t'''\n",
    "\tFunc Desc:\n",
    "\ttests whether the checkpoints stored are as expected\n",
    "\n",
    "\tInput:\n",
    "\n",
    "\tOutput:\n",
    "\n",
    "\t'''\n",
    "\tv = tf.get_variable(name='v', initializer=12, dtype=tf.int32)\n",
    "\tv1 = tf.assign_add(v, 1)\n",
    "\tsess = tf.Session()\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\tassert sess.run(v) == 12\n",
    "\tsess.run(v1)\n",
    "\tassert sess.run(v) == 13\n",
    "\tckpt = MRUCheckpoint('/tmp/checkpoints', sess, tf.global_variables())\n",
    "\tckpt.save()\n",
    "\n",
    "\tsess1 = tf.Session()\n",
    "\tsess1.run(tf.global_variables_initializer())\n",
    "\tassert sess1.run(v) == 12\n",
    "\tckpt1 = MRUCheckpoint('/tmp/checkpoints', sess1, tf.global_variables())\n",
    "\tckpt1.restore()\n",
    "\tassert sess1.run(v) == 13\n",
    "\tsess1.run(v1)\n",
    "\tassert sess1.run(v) == 14\n",
    "\n",
    "\tckpt2 = MRUCheckpoint('/tmp/bad-ckpt-path', sess1, tf.global_variables())\n",
    "\trestored = ckpt2.restore_if_checkpoint_exists()\n",
    "\tassert restored == False\n",
    "\n",
    "\trestored = ckpt1.restore_if_checkpoint_exists()\n",
    "\tassert restored\n",
    "\tassert sess1.run(v) == 13\n",
    "\n",
    "class BestCheckpoint():\n",
    "\tdef __init__(self, path, prefix, session, num_checkpoints, variables, global_step):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tInitializes the class member variables to find the Best checkpoint so far\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tpath\n",
    "\t\tprefix\n",
    "\t\tsession\n",
    "\t\tnum_checkpoints\n",
    "\t\tvariables\n",
    "\t\tglobal_step\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tself.ckpt_path = os.path.join(path, prefix)\n",
    "\t\t#self.ckpt_file = os.path.join(self.ckpt_path, 'checkpoint')\n",
    "\t\t#self.checkpoint_prefix = os.path.join(self.ckpt_path, prefix)\n",
    "\t\tself.sess = session\n",
    "\t\t# max_to_keep is None. Number of checkpoints is handled separately by BestCheckpointSaver\n",
    "\t\tself.saver = tf.train.Saver(variables, max_to_keep=None, save_relative_paths=True) \n",
    "\t\t# self.saver = tf.train.Saver()\n",
    "\t\tself.best_ckpt_saver = BestCheckpointSaver(\n",
    "\t\t\tsave_dir=self.ckpt_path,\n",
    "\t\t\tnum_to_keep=num_checkpoints,\n",
    "\t\t\tmaximize=True,\n",
    "\t\t\tsaver=self.saver\n",
    "\t\t\t)\n",
    "\t\tself.global_step = global_step\n",
    "\n",
    "\tdef save_if_best(self, metric):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tsave if the current checkpoint is the best so far\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tmetric\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tsaved = self.best_ckpt_saver.handle(metric, self.sess, self.global_step)\n",
    "\t\tpath = tf.train.latest_checkpoint(self.ckpt_path, 'checkpoint')\n",
    "\t\tif saved:\n",
    "\t\t\tprint('Saved new best checkpoint to path: ', path)\n",
    "\t\telse:\n",
    "\t\t\tprint('No new best checkpoint. Did not save a new best checkpoint. Last checkpointed file: ', path)\n",
    "\t\t\n",
    "\tdef restore_best_checkpoint(self):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tRestore the best checkpoint so far\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tbest_ckpt_file = get_best_checkpoint(self.ckpt_path, select_maximum_value=True)\n",
    "\t\tprint('Restoring best checkpoint from path: ', best_ckpt_file)\n",
    "\t\tself.saver.restore(self.sess, best_ckpt_file)\n",
    "\n",
    "\tdef restore_best_checkpoint_if_exists(self):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tRestore the best checkpoint so far only if it exists\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\ttry:\n",
    "\t\t\tself.restore_best_checkpoint()\n",
    "\t\t\treturn True\n",
    "\t\texcept ValueError as e:\n",
    "\t\t\tprint(str(e))\n",
    "\t\t\treturn False\n",
    "\n",
    "def test_best_ckpt():\n",
    "\t'''\n",
    "\tFunc Desc:\n",
    "\ttest for the best checkpoint so far\n",
    "\n",
    "\tInput:\n",
    "\n",
    "\tOutput:\n",
    "\n",
    "\t'''\n",
    "\tglobal_step = tf.get_variable(name='global_step', initializer=50, dtype=tf.int32)\n",
    "\tinc_global_step = tf.assign_add(global_step, 1)\n",
    "\tsess1 = tf.Session()\n",
    "\tsess2 = tf.Session()\n",
    "\t\n",
    "\tsess1.run(tf.global_variables_initializer())\n",
    "\tsess2.run(tf.global_variables_initializer())\n",
    "\n",
    "\t# We'll save using sess1 and restore in sess2\n",
    "\tbest_checkpoint_dir = '/tmp/best_ckpt_%.6f' % np.random.rand()\n",
    "\tbest1 = BestCheckpoint(best_checkpoint_dir, 'foo-bar', sess1, 3, tf.trainable_variables(), global_step)\n",
    "\tbest2 = BestCheckpoint(best_checkpoint_dir, 'foo-bar', sess2, 3, tf.trainable_variables(), global_step)\n",
    "\n",
    "\trestored = best2.restore_best_checkpoint_if_exists()\n",
    "\tassert not restored\n",
    "\n",
    "\tsess1.run(inc_global_step) ## 51\n",
    "\tbest1.save_if_best(0.1)\n",
    "\n",
    "\tassert sess2.run(global_step) == 50\n",
    "\trestored = best2.restore_best_checkpoint_if_exists()\n",
    "\tassert restored\n",
    "\tassert sess2.run(global_step) == 51\n",
    "\n",
    "\tsess1.run(inc_global_step) ## 52\n",
    "\tbest1.save_if_best(0.05)\n",
    "\n",
    "\tsess2.run(inc_global_step) # 52\n",
    "\tsess2.run(inc_global_step) # 53\n",
    "\tsess2.run(inc_global_step) # 54\n",
    "\tassert sess2.run(global_step) == 54\n",
    "\trestored = best2.restore_best_checkpoint_if_exists()\n",
    "\tassert restored\n",
    "\tassert sess2.run(global_step) == 51\n",
    "\n",
    "\tsess1.run(inc_global_step) ## 53\n",
    "\tbest1.save_if_best(0.2)\n",
    "\tsess1.run(inc_global_step) ## 54\n",
    "\tbest1.save_if_best(0.15)\n",
    "\n",
    "\tsess2.run(inc_global_step) # 52\n",
    "\tsess2.run(inc_global_step) # 53\n",
    "\tsess2.run(inc_global_step) # 54\n",
    "\tsess2.run(inc_global_step) # 55\n",
    "\tassert sess2.run(global_step) == 55\n",
    "\trestored = best2.restore_best_checkpoint_if_exists()\n",
    "\tassert restored\n",
    "\tassert sess2.run(global_step) == 53\n",
    "\n",
    "def test_checkmate():\n",
    "\t'''\n",
    "\tFunc Desc:\n",
    "\ttest whether the checkmate model is working fine\n",
    "\n",
    "\tInput:\n",
    "\n",
    "\tOutput:\n",
    "\n",
    "\t'''\n",
    "\tglobal_step = tf.get_variable(name='checkmate_global_step', initializer=12, dtype=tf.int32)\n",
    "\tinc_global_step_op = tf.assign_add(global_step, 1)\n",
    "\tsess = tf.Session()\n",
    "\tsess1 = tf.Session()\n",
    "\t\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\tsaver = tf.train.Saver(tf.trainable_variables(), max_to_keep=None)\n",
    "\t# saver = tf.train.Saver()\n",
    "\tbest_checkpoint_dir = '/tmp/best_ckpt_%.6f' % np.random.rand()\n",
    "\tbest_ckpt_saver = BestCheckpointSaver(\n",
    "\t\t\tsave_dir=best_checkpoint_dir,\n",
    "\t\t\tnum_to_keep=3,\n",
    "\t\t\tmaximize=True,\n",
    "\t\t\tsaver=saver\n",
    "\t\t\t)\n",
    "\n",
    "\taccuracy = 0.1 # 12\n",
    "\tbest_ckpt_saver.handle(accuracy, sess, global_step)\n",
    "\n",
    "\taccuracy = 0.2\n",
    "\tsess.run(inc_global_step_op) # 13\n",
    "\tbest_ckpt_saver.handle(accuracy, sess, global_step)\n",
    "\n",
    "\taccuracy = 0.05\n",
    "\tsess.run(inc_global_step_op) # 14\n",
    "\tbest_ckpt_saver.handle(accuracy, sess, global_step)\n",
    "\n",
    "\tckpt_path = get_best_checkpoint(best_checkpoint_dir, select_maximum_value=True)\n",
    "\tprint('Best ckpt path: ', ckpt_path)\n",
    "\tsaver.restore(sess1, ckpt_path)\n",
    "\tassert sess1.run(global_step) == 13\n",
    "\t\n",
    "\taccuracy = 0.12\n",
    "\tsess.run(inc_global_step_op) # 15\n",
    "\tbest_ckpt_saver.handle(accuracy, sess, global_step)\n",
    "\n",
    "\tckpt_path = get_best_checkpoint(best_checkpoint_dir, select_maximum_value=True)\n",
    "\tprint('Best ckpt path: ', ckpt_path)\n",
    "\tsaver.restore(sess1, ckpt_path)\n",
    "\tassert sess1.run(global_step) == 13\n",
    "\n",
    "\taccuracy = 0.45\n",
    "\tsess.run(inc_global_step_op) # 16\n",
    "\tbest_ckpt_saver.handle(accuracy, sess, global_step)\n",
    "\n",
    "\tckpt_path = get_best_checkpoint(best_checkpoint_dir, select_maximum_value=True)\n",
    "\tprint('Best ckpt path: ', ckpt_path)\n",
    "\tsaver.restore(sess1, ckpt_path)\n",
    "\tassert sess1.run(global_step) == 16\n",
    "\n",
    "\t# Now select lowest value\n",
    "\tckpt_path = get_best_checkpoint(best_checkpoint_dir, select_maximum_value=False)\n",
    "\tprint('Best ckpt path: ', ckpt_path)\n",
    "\tsaver.restore(sess1, ckpt_path)\n",
    "\tassert sess1.run(global_step) == 15\n",
    "\n",
    "# Loading of checkpoints happens only once - at the end of HLSModel initialization.\n",
    "#\n",
    "# Saving of checkpoints happens during training. We have only one MRU checkpoint saver \n",
    "# We have one best checkpoint saver per train mode type\n",
    "class CheckpointsFactory:\n",
    "\tdef __init__(self, sess, global_steps):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tInitializes the class with the arguments\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tsess \n",
    "\t\tglobal_steps\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tself.best_savers = {}\n",
    "\t\tself.initialize_savers(sess, global_steps)\n",
    "\n",
    "\tdef get_best_saver(self, train_mode):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tget the best saved checkpoints\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tTrain_mode - the mode of training\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\treturn self.best_savers[train_mode]\n",
    "\n",
    "\tdef initialize_savers(self, sess, global_steps):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tInitialize the required savers\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tsess\n",
    "\t\tglobal_steps\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tfor mode in train_modes:\n",
    "\t\t\tself.init_saver(sess, mode, global_steps)\n",
    "\n",
    "\tdef init_saver(self, sess, mode, global_steps):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tInitialize the required savers with the given mode\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tsess\n",
    "\t\tmode\n",
    "\t\tglobal_steps\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tckpt_dir = checkpoint_dir\n",
    "\t\tself.best_savers[mode] = BestCheckpoint(ckpt_dir, mode, sess,\n",
    "\t\t\t\tnum_checkpoints, tf.global_variables(), global_steps[mode])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttest_best_ckpt()\n",
    "\ttest_checkmate()\n",
    "\ttest_checkpoint()\n",
    "\ttest_mru_checkpoints(num_to_keep=1)\n",
    "\ttest_mru_checkpoints(num_to_keep=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29b535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.Implyloss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d0ceb92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'examples/SMS_SPAM/data_pipeline/sms_pickle_L.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c489b653866a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid run mode '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     data_feeder = DataFeeder(d_pickle, \n\u001b[0m\u001b[1;32m     97\u001b[0m                              \u001b[0mU_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                              \u001b[0mvalidation_pickle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SEM6/RnD/spear/spear/Implyloss/my_data_feeders.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, d_pickle, U_pickle, validation_pickle, map_json, shuffle_batches, num_load_d, num_load_U, num_classes, f_d_class_sampling, min_rule_coverage, rule_classes, num_load_validation, f_d_batch_size, f_d_U_batch_size, test_w_batch_size, out_dir)\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_load_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_U\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mU_pickle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_load_U\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SEM6/RnD/spear/spear/Implyloss/my_data_feeder_utils.py\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(fname, jname, num_load)\u001b[0m\n\u001b[1;32m     45\u001b[0m     '''\n\u001b[1;32m     46\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading from hoff '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.astype(np.int32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'examples/SMS_SPAM/data_pipeline/sms_pickle_L.pkl'"
     ]
    }
   ],
   "source": [
    "# from my_data_feeders import DataFeeder\n",
    "\n",
    "# from my_model import HighLevelSupervisionNetwork\n",
    "# import my_networks\n",
    "from spear.Implyloss import *\n",
    "import numpy as np\n",
    "import sys, os, shutil\n",
    "\n",
    "checkpoint_dir =  './checkpoint'\n",
    "# data_dir = \"/home/parth/Desktop/SEM6/RnD/Learning-From-Rules/data/TREC\" # Directory containing data pickles\n",
    "# data_dir = \"/home/parth/Desktop/SEM6/RnD/spear/examples/SMS_SPAM/data_pipeline/\"\n",
    "data_dir = \"../../examples/SMS_SPAM/data_pipeline/\"\n",
    "inference_output_dir = './inference_output/'\n",
    "log_dir = './logs'\n",
    "metric_pickle_dir = './met_pickl/'\n",
    "tensorboard_dir =  './tensorboard'\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(inference_output_dir):\n",
    "    os.makedirs(inference_output_dir)\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(metric_pickle_dir):\n",
    "    os.makedirs(metric_pickle_dir)\n",
    "\n",
    "if not os.path.exists(tensorboard_dir):\n",
    "    os.makedirs(tensorboard_dir)\n",
    "\n",
    "checkpoint_load_mode = 'mru' # Which kind of checkpoint to restore from. Possible options are mru: Most recently saved checkpoint. Use this to continue a run f_d, f_d_U: Use these to load the best checkpoint from these runs \n",
    "# d_pickle = data_dir+\"d_processed.p\"\n",
    "d_pickle = data_dir+\"sms_pickle_L.pkl\"\n",
    "dropout_keep_prob =  0.8\n",
    "early_stopping_p = 20 # early stopping patience (in epochs)\n",
    "f_d_adam_lr =  0.0003 # default = 0.01\n",
    "f_d_batch_size = 16\n",
    "f_d_class_sampling = [10,10] # Comma-separated list of number of times each d instance should be sampled depending on its class for training f on d. Size of list must equal number of classes.\n",
    "f_d_epochs = 4 # default = 2\n",
    "f_d_metrics_pickle = metric_pickle_dir+\"metrics_train_f_on_d.p\"\n",
    "f_d_primary_metric = 'accuracy' #'f1_score_1' # Metric for best checkpoint computation. The best metrics pickle will also be stored on this basis. Valid values are: accuracy: overall accuracy. f1_score_1: f1_score of class 1. avg_f1_score: average of all classes f1_score \n",
    "f_d_U_adam_lr =  0.0003 # default = 0.01\n",
    "f_d_U_batch_size = 32\n",
    "f_d_U_epochs = 4 # default = 2  \n",
    "f_d_U_metrics_pickle = metric_pickle_dir+\"metrics_train_f_on_d_U.p\"\n",
    "f_infer_out_pickle = inference_output_dir+\"infer_f.p\" # output file name for any inference that was ran on f (classification) network\n",
    "gamma = 0.1 # weighting factor for loss on U used in implication, pr_loss, snorkel, generalized cross entropy etc. \n",
    "lamda = 0.1\n",
    "min_rule_coverage = 0 # Minimum coverage of a rule in U in order to include it in co-training. Rules which have coverage less than this are assigned a constant weight of 1.0.\n",
    "mode = \"learn2reweight\" # \"learn2reweight\" / \"implication\" / \"pr_loss\" / \"label_snorkel\" / \"gcross\" / \"gcross_snorkel\" / \"f_d\" \n",
    "test_mode = \"\" # \"\" / test_f\" / \"test_w\" / \"test_all\"\n",
    "num_classes = 2 # can be 0. Number of classes. If 0, this will be dynamically determined using max of labels in 'd'.\n",
    "num_load_d = None # can be 0. Number of instances to load from d. If 0 load all.\n",
    "num_load_U = None # can be 0. Number of instances to load from U. If 0 load all.\n",
    "num_load_validation = None # can be 0. Number of instances to load from validation. If 0 load all.\n",
    "q = \"1\"\n",
    "rule_classes = None # Comma-separated list of the classes predicted by each rule if string is empty, rule classes are determined from data associated with rule firings.\n",
    "shuffle_batches = True # Don't shuffle batches. Useful for debugging and stepping through batch by batch\n",
    "test_w_batch_size = 1000\n",
    "# U_pickle = data_dir+\"U_processed.p\"\n",
    "U_pickle = data_dir+\"sms_pickle_U.pkl\"\n",
    "use_joint_f_w = False # whether to utilize w network during inference\n",
    "# validation_pickle = data_dir+\"validation_processed.p\"\n",
    "validation_pickle = data_dir+\"sms_pickle_V.pkl\"\n",
    "w_infer_out_pickle = inference_output_dir+\"infer_w.p\" # output file name for any inference that was ran on w (rule) network\n",
    "json_file = data_dir+\"sms_json.json\"\n",
    "\n",
    "output_dir = \"./\" + str(mode) + \"_\" + str(gamma) + \"_\" + str(lamda) + \"_\" + str(q)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if test_mode==\"\":\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        shutil.rmtree(checkpoint_dir, ignore_errors=True)    \n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# number of input dir - 1 (data_dir)\n",
    "# number of output dir - 6 (checkpoint, inference_output, log_dir, metric_pickle, output, tensorboard)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if(str(test_mode)==\"\"):\n",
    "        output_text_file=log_dir + \"/\" + str(mode) + \"_\" + str(gamma) + \"_\" + str(lamda) + \"_\" + str(q)+\".txt\"\n",
    "    else:    \n",
    "        output_text_file=log_dir + \"/\" + str(test_mode) + \"_\" + str(mode) + \"_\" + str(gamma) + \"_\" + str(lamda) + \"_\" + str(q)+\".txt\"\n",
    "    sys.stdout = open(output_text_file,\"w\")\n",
    "    if(test_mode!=\"\"):\n",
    "        mode = test_mode\n",
    "    if mode not in ['learn2reweight', 'implication', 'f_d', 'pr_loss', 'gcross',  'label_snorkel', 'pure_snorkel', 'gcross_snorkel', 'test_f', 'test_w', 'test_all']:\n",
    "        raise ValueError('Invalid run mode ' + mode)\n",
    "\n",
    "    data_feeder = DataFeeder(d_pickle, \n",
    "                             U_pickle, \n",
    "                             validation_pickle,\n",
    "                             json_file,\n",
    "                             shuffle_batches, \n",
    "                             num_load_d, \n",
    "                             num_load_U, \n",
    "                             num_classes, \n",
    "                             f_d_class_sampling, \n",
    "                             min_rule_coverage, \n",
    "                             rule_classes, \n",
    "                             num_load_validation, \n",
    "                             f_d_batch_size, \n",
    "                             f_d_U_batch_size, \n",
    "                             test_w_batch_size,\n",
    "                             out_dir=output_dir)\n",
    "\n",
    "    num_features, num_classes, num_rules, num_rules_to_train = data_feeder.get_features_classes_rules()\n",
    "    print(\"Number of features: \", num_features)\n",
    "    print(\"Number of classes: \",num_classes)\n",
    "    print(\"Print num of rules to train: \", num_rules_to_train)\n",
    "    print(\"Print num of rules: \", num_rules)\n",
    "    print(\"\\n\\n\")\n",
    "    rule_classes = data_feeder.rule_classes\n",
    "    w_network = my_networks.w_network_fully_connected #rule network - CHANGE config in w_network_fully_connected of my_networks - DONE\n",
    "    f_network = my_networks.f_network_fully_connected #classification network - CHANGE config in f_network_fully_connected of my_networks - DONE\n",
    "    hls = HighLevelSupervisionNetwork(\n",
    "            num_features,\n",
    "            num_classes,\n",
    "            num_rules,\n",
    "            num_rules_to_train,\n",
    "            rule_classes,\n",
    "            w_network,\n",
    "            f_network,\n",
    "            f_d_epochs, \n",
    "            f_d_U_epochs, \n",
    "            f_d_adam_lr, \n",
    "            f_d_U_adam_lr, \n",
    "            dropout_keep_prob, \n",
    "            f_d_metrics_pickle, \n",
    "            f_d_U_metrics_pickle, \n",
    "            early_stopping_p, \n",
    "            f_d_primary_metric, \n",
    "            mode, \n",
    "            data_dir, \n",
    "            tensorboard_dir, \n",
    "            checkpoint_dir, \n",
    "            checkpoint_load_mode, \n",
    "            gamma, \n",
    "            lamda,\n",
    "            raw_d_x=data_feeder.raw_d.x, #instances from the \"d\" set\n",
    "            raw_d_L=data_feeder.raw_d.L) #labels from the \"d\" set\n",
    "\n",
    "    # Output 3 digits after decimal point in numpy arrays\n",
    "    float_formatter = lambda x: \"%.3f\" % x\n",
    "    np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "\n",
    "    print('Run mode is ' + mode)\n",
    "    if mode == 'f_d':\n",
    "        print('training f on d')\n",
    "        hls.train.train_f_on_d(data_feeder, f_d_epochs)\n",
    "    elif mode == 'implication':\n",
    "        print(\"begin Implication loss training\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='implication')\n",
    "        print(\" Implication loss training end\")\n",
    "    elif mode == 'pr_loss':\n",
    "        print(\"begin pr_loss training\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='pr_loss')\n",
    "        print(\"pr_loss training end\")\n",
    "    elif mode == 'gcross': # majority_label\n",
    "        print(\"gcross\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='gcross')\n",
    "    elif mode == 'gcross_snorkel':\n",
    "        print(\"gcross_snorkel\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='gcross_snorkel')\n",
    "    elif mode == 'learn2reweight':\n",
    "        print('learn2reweight')\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='learn2reweight')\n",
    "    elif mode == 'pure_snorkel':\n",
    "        print(\"pure_snorkel\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='pure_snorkel')\n",
    "    elif mode == 'label_snorkel':\n",
    "        print(\"label_snorkel\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='label_snorkel')\n",
    "    elif mode == 'test_f':\n",
    "        print('Running test_f')\n",
    "        hls.test.test_f(data_feeder, log_output=True, \n",
    "                        save_filename=f_infer_out_pickle, \n",
    "                        use_joint_f_w=use_joint_f_w)\n",
    "    elif mode == 'test_w':\n",
    "        print('Running test_w')\n",
    "        hls.test.test_w(data_feeder, log_output=True, save_filename=w_infer_out_pickle+\"_test\")\n",
    "    elif mode == 'test_all':\n",
    "        print('Running all tests')\n",
    "        print('\\ninference on f network ...\\n')\n",
    "        hls.test.test_f(data_feeder, log_output=True, \n",
    "                        save_filename=f_infer_out_pickle,\n",
    "                        use_joint_f_w=use_joint_f_w)\n",
    "        print('\\ninference on w network...')\n",
    "        print('we only test on instances covered by atleast one rule\\n')\n",
    "        hls.test.test_w(data_feeder, log_output=True, save_filename=w_infer_out_pickle+\"_test\")\n",
    "    else:\n",
    "        assert not \"Invalid mode string: %s\" % mode\n",
    "\n",
    "    sys.stdout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
