{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffd61e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944fe4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_data_feeders\n",
    "import collections\n",
    "\n",
    "f_d = 'f_d'\n",
    "f_d_U = 'f_d_U'\n",
    "test_w = 'test_w'\n",
    "\n",
    "train_modes = [f_d, f_d_U]\n",
    "\n",
    "F_d_U_Data = collections.namedtuple('GMMDataF_d_U', 'x l m L d r')\n",
    "F_d_Data = collections.namedtuple('GMMDataF_d', 'x labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2758a604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/parth/.local/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "from spear.Implyloss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "016a1991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint file does not exist\n",
      "INFO:tensorflow:best.ckpt-51 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-51\n",
      "Restoring best checkpoint from path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:best.ckpt-52 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-52\n",
      "Restoring best checkpoint from path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-51\n",
      "INFO:tensorflow:best.ckpt-53 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-53\n",
      "INFO:tensorflow:best.ckpt-54 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Saved new best checkpoint to path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-54\n",
      "Restoring best checkpoint from path:  /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-53\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.795270/foo-bar/best.ckpt-53\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.354182/best.ckpt-12 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.354182/best.ckpt-13 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.354182/best.ckpt-14 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Best ckpt path:  /tmp/best_ckpt_0.354182/best.ckpt-13\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.354182/best.ckpt-13\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.354182/best.ckpt-15 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Best ckpt path:  /tmp/best_ckpt_0.354182/best.ckpt-13\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.354182/best.ckpt-13\n",
      "INFO:tensorflow:/tmp/best_ckpt_0.354182/best.ckpt-16 is not in all_model_checkpoint_paths. Manually adding it.\n",
      "Best ckpt path:  /tmp/best_ckpt_0.354182/best.ckpt-16\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.354182/best.ckpt-16\n",
      "Best ckpt path:  /tmp/best_ckpt_0.354182/best.ckpt-15\n",
      "INFO:tensorflow:Restoring parameters from /tmp/best_ckpt_0.354182/best.ckpt-15\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints/hls-model\n",
      "Restoring checkpoint from path:  /tmp/checkpoints/hls-model\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints/hls-model\n",
      "Restoring checkpoint from path:  /tmp/checkpoints/hls-model\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints/hls-model\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.741876/hls-model-11\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.741876/hls-model-11\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.741876/hls-model-11\n",
      "WARNING:tensorflow:From /home/parth/.local/lib/python3.8/site-packages/tensorflow/python/training/saver.py:968: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.741876/hls-model-5\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.741876/hls-model-5\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.741876/hls-model-5\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.582258/hls-model-11\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.582258/hls-model-11\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.582258/hls-model-11\n",
      "Saved MRU checkpoint to path:  /tmp/checkpoints_0.582258/hls-model-5\n",
      "Restoring checkpoint from path:  /tmp/checkpoints_0.582258/hls-model-5\n",
      "INFO:tensorflow:Restoring parameters from /tmp/checkpoints_0.582258/hls-model-5\n"
     ]
    }
   ],
   "source": [
    "#my_checkpoints\n",
    "# import tensorflow as tf\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# from my_checkmate import BestCheckpointSaver, get_best_checkpoint\n",
    "# from my_data_types import train_modes\n",
    "checkpoint_dir = \"./checkpoint\"\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "\tos.makedirs(checkpoint_dir)\n",
    "\n",
    "num_checkpoints = 1 # Number of checkpoints to keep around\n",
    "\n",
    "# Keeps only the most recently saved checkpoint\n",
    "#\n",
    "# max_to_keep is deliberatly set to 1 in order to provide for the case when more recent checkpoint\n",
    "# has a smaller global_step. tf.train.Saver() orders by global_step.\n",
    "\n",
    "class MRUCheckpoint():\n",
    "\tdef __init__(self, path, session, variables):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tInitializes the class variables\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tpath - file path\n",
    "\t\tsession \n",
    "\t\tvariables\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tself.ckpt_path = path\n",
    "\t\tself.ckpt_file = os.path.join(path, 'checkpoint')\n",
    "\t\tself.checkpoint_prefix = os.path.join(self.ckpt_path, 'hls-model')\n",
    "\t\tself.sess = session\n",
    "\t\t# max_to_keep\n",
    "\t\tself.saver = tf.train.Saver(variables, max_to_keep=1)\n",
    "\t\t# self.saver = tf.train.Saver()\n",
    "\n",
    "\tdef save(self, global_step=None):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tsaves the obtained checkpoint\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tglobal step (Default - none)\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tpath = self.saver.save(self.sess, self.checkpoint_prefix, global_step)\n",
    "\t\tprint('Saved MRU checkpoint to path: ', path)\n",
    "\t\t\n",
    "\tdef restore(self):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tRestores the last checkpoint\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tlast_checkpoint = tf.train.latest_checkpoint(self.ckpt_path, 'checkpoint')\n",
    "\t\t#if self.saver.last_checkpoints:\n",
    "\t\t#    last_checkpoint = self.saver.last_checkpoints[0]\n",
    "\t\t#    print('All saved checkpoints: ', self.saver.last_checkpoints)\n",
    "\t\t#else:\n",
    "\t\tif not last_checkpoint:\n",
    "\t\t\tlast_checkpoint = self.checkpoint_prefix\n",
    "\n",
    "\t\tprint('Restoring checkpoint from path: ', last_checkpoint)\n",
    "\t\tself.saver.restore(self.sess, last_checkpoint)\n",
    "\n",
    "\tdef restore_if_checkpoint_exists(self):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tchecks if there exists any checkpoint for the file \n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\n",
    "\t\tOutput:\n",
    "\t\tBoolean (True or False)\n",
    "\t\t'''\n",
    "\t\tif os.path.exists(self.ckpt_file):\n",
    "\t\t\tself.restore()\n",
    "\t\t\treturn True\n",
    "\t\treturn False\n",
    "\n",
    "def test_mru_checkpoints(num_to_keep):\n",
    "\t'''\n",
    "\tFunc Desc:\n",
    "\tRuns different sessions while changing the checkpoint number that is currently being worked with and tests the same\n",
    "\t\n",
    "\tInput:\n",
    "\tnum_to_keep(int) - a limit on the size of the global step for checkpoint traversal\n",
    "\n",
    "\tOutput:\n",
    "\n",
    "\t'''\n",
    "\tglobal_step = tf.get_variable(name='mru_global_step_%d' % num_to_keep, initializer=10, dtype=tf.int32)\n",
    "\tinc = tf.assign_add(global_step, 1)\n",
    "\tsess = tf.Session()\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\tassert sess.run(global_step) == 10\n",
    "\n",
    "\tsess.run(inc)\n",
    "\tassert sess.run(global_step) == 11\n",
    "\tckpt_path = '/tmp/checkpoints_%.6f' % np.random.rand()\n",
    "\tckpt = MRUCheckpoint(ckpt_path, sess, tf.global_variables())\n",
    "\tckpt.save(global_step)\n",
    "\n",
    "\tsess.run(inc)\n",
    "\tassert sess.run(global_step) == 12\n",
    "\n",
    "\tckpt.restore_if_checkpoint_exists()\n",
    "\tassert sess.run(global_step) == 11\n",
    "\n",
    "\tassgn_op = tf.assign(global_step, 5)\n",
    "\tsess.run(assgn_op)\n",
    "\tassert sess.run(global_step) == 5\n",
    "\tckpt.save(global_step)\n",
    "\n",
    "\tsess.run(inc)\n",
    "\tsess.run(inc)\n",
    "\tassert sess.run(global_step) == 7\n",
    "\n",
    "\tckpt.restore_if_checkpoint_exists()\n",
    "\tassert sess.run(global_step) == 5\n",
    "\n",
    "def test_checkpoint():\n",
    "\t'''\n",
    "\tFunc Desc:\n",
    "\ttests whether the checkpoints stored are as expected\n",
    "\n",
    "\tInput:\n",
    "\n",
    "\tOutput:\n",
    "\n",
    "\t'''\n",
    "\tv = tf.get_variable(name='v', initializer=12, dtype=tf.int32)\n",
    "\tv1 = tf.assign_add(v, 1)\n",
    "\tsess = tf.Session()\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\tassert sess.run(v) == 12\n",
    "\tsess.run(v1)\n",
    "\tassert sess.run(v) == 13\n",
    "\tckpt = MRUCheckpoint('/tmp/checkpoints', sess, tf.global_variables())\n",
    "\tckpt.save()\n",
    "\n",
    "\tsess1 = tf.Session()\n",
    "\tsess1.run(tf.global_variables_initializer())\n",
    "\tassert sess1.run(v) == 12\n",
    "\tckpt1 = MRUCheckpoint('/tmp/checkpoints', sess1, tf.global_variables())\n",
    "\tckpt1.restore()\n",
    "\tassert sess1.run(v) == 13\n",
    "\tsess1.run(v1)\n",
    "\tassert sess1.run(v) == 14\n",
    "\n",
    "\tckpt2 = MRUCheckpoint('/tmp/bad-ckpt-path', sess1, tf.global_variables())\n",
    "\trestored = ckpt2.restore_if_checkpoint_exists()\n",
    "\tassert restored == False\n",
    "\n",
    "\trestored = ckpt1.restore_if_checkpoint_exists()\n",
    "\tassert restored\n",
    "\tassert sess1.run(v) == 13\n",
    "\n",
    "class BestCheckpoint():\n",
    "\tdef __init__(self, path, prefix, session, num_checkpoints, variables, global_step):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tInitializes the class member variables to find the Best checkpoint so far\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tpath\n",
    "\t\tprefix\n",
    "\t\tsession\n",
    "\t\tnum_checkpoints\n",
    "\t\tvariables\n",
    "\t\tglobal_step\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tself.ckpt_path = os.path.join(path, prefix)\n",
    "\t\t#self.ckpt_file = os.path.join(self.ckpt_path, 'checkpoint')\n",
    "\t\t#self.checkpoint_prefix = os.path.join(self.ckpt_path, prefix)\n",
    "\t\tself.sess = session\n",
    "\t\t# max_to_keep is None. Number of checkpoints is handled separately by BestCheckpointSaver\n",
    "\t\tself.saver = tf.train.Saver(variables, max_to_keep=None, save_relative_paths=True) \n",
    "\t\t# self.saver = tf.train.Saver()\n",
    "\t\tself.best_ckpt_saver = BestCheckpointSaver(\n",
    "\t\t\tsave_dir=self.ckpt_path,\n",
    "\t\t\tnum_to_keep=num_checkpoints,\n",
    "\t\t\tmaximize=True,\n",
    "\t\t\tsaver=self.saver\n",
    "\t\t\t)\n",
    "\t\tself.global_step = global_step\n",
    "\n",
    "\tdef save_if_best(self, metric):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tsave if the current checkpoint is the best so far\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tmetric\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tsaved = self.best_ckpt_saver.handle(metric, self.sess, self.global_step)\n",
    "\t\tpath = tf.train.latest_checkpoint(self.ckpt_path, 'checkpoint')\n",
    "\t\tif saved:\n",
    "\t\t\tprint('Saved new best checkpoint to path: ', path)\n",
    "\t\telse:\n",
    "\t\t\tprint('No new best checkpoint. Did not save a new best checkpoint. Last checkpointed file: ', path)\n",
    "\t\t\n",
    "\tdef restore_best_checkpoint(self):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tRestore the best checkpoint so far\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tbest_ckpt_file = get_best_checkpoint(self.ckpt_path, select_maximum_value=True)\n",
    "\t\tprint('Restoring best checkpoint from path: ', best_ckpt_file)\n",
    "\t\tself.saver.restore(self.sess, best_ckpt_file)\n",
    "\n",
    "\tdef restore_best_checkpoint_if_exists(self):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tRestore the best checkpoint so far only if it exists\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\ttry:\n",
    "\t\t\tself.restore_best_checkpoint()\n",
    "\t\t\treturn True\n",
    "\t\texcept ValueError as e:\n",
    "\t\t\tprint(str(e))\n",
    "\t\t\treturn False\n",
    "\n",
    "def test_best_ckpt():\n",
    "\t'''\n",
    "\tFunc Desc:\n",
    "\ttest for the best checkpoint so far\n",
    "\n",
    "\tInput:\n",
    "\n",
    "\tOutput:\n",
    "\n",
    "\t'''\n",
    "\tglobal_step = tf.get_variable(name='global_step', initializer=50, dtype=tf.int32)\n",
    "\tinc_global_step = tf.assign_add(global_step, 1)\n",
    "\tsess1 = tf.Session()\n",
    "\tsess2 = tf.Session()\n",
    "\t\n",
    "\tsess1.run(tf.global_variables_initializer())\n",
    "\tsess2.run(tf.global_variables_initializer())\n",
    "\n",
    "\t# We'll save using sess1 and restore in sess2\n",
    "\tbest_checkpoint_dir = '/tmp/best_ckpt_%.6f' % np.random.rand()\n",
    "\tbest1 = BestCheckpoint(best_checkpoint_dir, 'foo-bar', sess1, 3, tf.trainable_variables(), global_step)\n",
    "\tbest2 = BestCheckpoint(best_checkpoint_dir, 'foo-bar', sess2, 3, tf.trainable_variables(), global_step)\n",
    "\n",
    "\trestored = best2.restore_best_checkpoint_if_exists()\n",
    "\tassert not restored\n",
    "\n",
    "\tsess1.run(inc_global_step) ## 51\n",
    "\tbest1.save_if_best(0.1)\n",
    "\n",
    "\tassert sess2.run(global_step) == 50\n",
    "\trestored = best2.restore_best_checkpoint_if_exists()\n",
    "\tassert restored\n",
    "\tassert sess2.run(global_step) == 51\n",
    "\n",
    "\tsess1.run(inc_global_step) ## 52\n",
    "\tbest1.save_if_best(0.05)\n",
    "\n",
    "\tsess2.run(inc_global_step) # 52\n",
    "\tsess2.run(inc_global_step) # 53\n",
    "\tsess2.run(inc_global_step) # 54\n",
    "\tassert sess2.run(global_step) == 54\n",
    "\trestored = best2.restore_best_checkpoint_if_exists()\n",
    "\tassert restored\n",
    "\tassert sess2.run(global_step) == 51\n",
    "\n",
    "\tsess1.run(inc_global_step) ## 53\n",
    "\tbest1.save_if_best(0.2)\n",
    "\tsess1.run(inc_global_step) ## 54\n",
    "\tbest1.save_if_best(0.15)\n",
    "\n",
    "\tsess2.run(inc_global_step) # 52\n",
    "\tsess2.run(inc_global_step) # 53\n",
    "\tsess2.run(inc_global_step) # 54\n",
    "\tsess2.run(inc_global_step) # 55\n",
    "\tassert sess2.run(global_step) == 55\n",
    "\trestored = best2.restore_best_checkpoint_if_exists()\n",
    "\tassert restored\n",
    "\tassert sess2.run(global_step) == 53\n",
    "\n",
    "def test_checkmate():\n",
    "\t'''\n",
    "\tFunc Desc:\n",
    "\ttest whether the checkmate model is working fine\n",
    "\n",
    "\tInput:\n",
    "\n",
    "\tOutput:\n",
    "\n",
    "\t'''\n",
    "\tglobal_step = tf.get_variable(name='checkmate_global_step', initializer=12, dtype=tf.int32)\n",
    "\tinc_global_step_op = tf.assign_add(global_step, 1)\n",
    "\tsess = tf.Session()\n",
    "\tsess1 = tf.Session()\n",
    "\t\n",
    "\tsess.run(tf.global_variables_initializer())\n",
    "\tsaver = tf.train.Saver(tf.trainable_variables(), max_to_keep=None)\n",
    "\t# saver = tf.train.Saver()\n",
    "\tbest_checkpoint_dir = '/tmp/best_ckpt_%.6f' % np.random.rand()\n",
    "\tbest_ckpt_saver = BestCheckpointSaver(\n",
    "\t\t\tsave_dir=best_checkpoint_dir,\n",
    "\t\t\tnum_to_keep=3,\n",
    "\t\t\tmaximize=True,\n",
    "\t\t\tsaver=saver\n",
    "\t\t\t)\n",
    "\n",
    "\taccuracy = 0.1 # 12\n",
    "\tbest_ckpt_saver.handle(accuracy, sess, global_step)\n",
    "\n",
    "\taccuracy = 0.2\n",
    "\tsess.run(inc_global_step_op) # 13\n",
    "\tbest_ckpt_saver.handle(accuracy, sess, global_step)\n",
    "\n",
    "\taccuracy = 0.05\n",
    "\tsess.run(inc_global_step_op) # 14\n",
    "\tbest_ckpt_saver.handle(accuracy, sess, global_step)\n",
    "\n",
    "\tckpt_path = get_best_checkpoint(best_checkpoint_dir, select_maximum_value=True)\n",
    "\tprint('Best ckpt path: ', ckpt_path)\n",
    "\tsaver.restore(sess1, ckpt_path)\n",
    "\tassert sess1.run(global_step) == 13\n",
    "\t\n",
    "\taccuracy = 0.12\n",
    "\tsess.run(inc_global_step_op) # 15\n",
    "\tbest_ckpt_saver.handle(accuracy, sess, global_step)\n",
    "\n",
    "\tckpt_path = get_best_checkpoint(best_checkpoint_dir, select_maximum_value=True)\n",
    "\tprint('Best ckpt path: ', ckpt_path)\n",
    "\tsaver.restore(sess1, ckpt_path)\n",
    "\tassert sess1.run(global_step) == 13\n",
    "\n",
    "\taccuracy = 0.45\n",
    "\tsess.run(inc_global_step_op) # 16\n",
    "\tbest_ckpt_saver.handle(accuracy, sess, global_step)\n",
    "\n",
    "\tckpt_path = get_best_checkpoint(best_checkpoint_dir, select_maximum_value=True)\n",
    "\tprint('Best ckpt path: ', ckpt_path)\n",
    "\tsaver.restore(sess1, ckpt_path)\n",
    "\tassert sess1.run(global_step) == 16\n",
    "\n",
    "\t# Now select lowest value\n",
    "\tckpt_path = get_best_checkpoint(best_checkpoint_dir, select_maximum_value=False)\n",
    "\tprint('Best ckpt path: ', ckpt_path)\n",
    "\tsaver.restore(sess1, ckpt_path)\n",
    "\tassert sess1.run(global_step) == 15\n",
    "\n",
    "# Loading of checkpoints happens only once - at the end of HLSModel initialization.\n",
    "#\n",
    "# Saving of checkpoints happens during training. We have only one MRU checkpoint saver \n",
    "# We have one best checkpoint saver per train mode type\n",
    "class CheckpointsFactory:\n",
    "\tdef __init__(self, sess, global_steps):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tInitializes the class with the arguments\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tsess \n",
    "\t\tglobal_steps\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tself.best_savers = {}\n",
    "\t\tself.initialize_savers(sess, global_steps)\n",
    "\n",
    "\tdef get_best_saver(self, train_mode):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tget the best saved checkpoints\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tTrain_mode - the mode of training\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\treturn self.best_savers[train_mode]\n",
    "\n",
    "\tdef initialize_savers(self, sess, global_steps):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tInitialize the required savers\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tsess\n",
    "\t\tglobal_steps\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tfor mode in train_modes:\n",
    "\t\t\tself.init_saver(sess, mode, global_steps)\n",
    "\n",
    "\tdef init_saver(self, sess, mode, global_steps):\n",
    "\t\t'''\n",
    "\t\tFunc Desc:\n",
    "\t\tInitialize the required savers with the given mode\n",
    "\n",
    "\t\tInput:\n",
    "\t\tself\n",
    "\t\tsess\n",
    "\t\tmode\n",
    "\t\tglobal_steps\n",
    "\n",
    "\t\tOutput:\n",
    "\n",
    "\t\t'''\n",
    "\t\tckpt_dir = checkpoint_dir\n",
    "\t\tself.best_savers[mode] = BestCheckpoint(ckpt_dir, mode, sess,\n",
    "\t\t\t\tnum_checkpoints, tf.global_variables(), global_steps[mode])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\ttest_best_ckpt()\n",
    "\ttest_checkmate()\n",
    "\ttest_checkpoint()\n",
    "\ttest_mru_checkpoints(num_to_keep=1)\n",
    "\ttest_mru_checkpoints(num_to_keep=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a29b535d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spear.Implyloss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d0ceb92",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Variable f_network/dense/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"../../spear/Implyloss/my_networks.py\", line 88, in f_network_fully_connected\n    cur_layer = tf.layers.dense(prev_layer, num_neurons,\n  File \"../../spear/Implyloss/my_model.py\", line 172, in make_f_d_train_ops\n    self.f_d_logits = self.f_network(f_dict, self.num_classes,\n  File \"../../spear/Implyloss/my_model.py\", line 100, in __init__\n    self.make_f_d_train_ops()\n  File \"<ipython-input-6-7277876691ff>\", line 122, in <module>\n    hls = HighLevelSupervisionNetwork(\n  File \"/home/parth/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-7277876691ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0mw_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_networks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_network_fully_connected\u001b[0m \u001b[0;31m#rule network - CHANGE config in w_network_fully_connected of my_networks - DONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0mf_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_networks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_network_fully_connected\u001b[0m \u001b[0;31m#classification network - CHANGE config in f_network_fully_connected of my_networks - DONE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     hls = HighLevelSupervisionNetwork(\n\u001b[0m\u001b[1;32m    123\u001b[0m             \u001b[0mnum_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mnum_classes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SEM6/RnD/spear/spear/Implyloss/my_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_features, num_classes, num_rules, num_rules_to_train, rule_classes, w_network, f_network, f_d_epochs, f_d_U_epochs, f_d_adam_lr, f_d_U_adam_lr, dropout_keep_prob, f_d_metrics_pickle, f_d_U_metrics_pickle, early_stopping_p, f_d_primary_metric, mode, data_dir, tensorboard_dir, checkpoint_dir, checkpoint_load_mode, gamma, lamda, raw_d_x, raw_d_L)\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHLSTest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_f_d_train_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_f_d_U_train_ops\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SEM6/RnD/spear/spear/Implyloss/my_model.py\u001b[0m in \u001b[0;36mmake_f_d_train_ops\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mf_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'x'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_d_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m \t\tself.f_d_logits = self.f_network(f_dict, self.num_classes,\n\u001b[0m\u001b[1;32m    173\u001b[0m \t\t\t\tdropout_keep_prob=self.dropout_keep_prob)\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/SEM6/RnD/spear/spear/Implyloss/my_networks.py\u001b[0m in \u001b[0;36mf_network_fully_connected\u001b[0;34m(f_var_scope, f_dict, num_classes, reuse, ph_vars, dropout_keep_prob)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mprev_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neurons\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                 cur_layer = tf.layers.dense(prev_layer, num_neurons,\n\u001b[0m\u001b[1;32m     89\u001b[0m                         activation=tf.nn.relu)\n\u001b[1;32m     90\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnetwork_dropout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/legacy_tf_layers/core.py\u001b[0m in \u001b[0;36mdense\u001b[0;34m(inputs, units, activation, use_bias, kernel_initializer, bias_initializer, kernel_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, bias_constraint, trainable, name, reuse)\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0m_scope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 _reuse=reuse)\n\u001b[0;32m--> 188\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1720\u001b[0m                   \u001b[0;34m'will be removed in a future version. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1721\u001b[0m                   'Please use `layer.__call__` method instead.')\n\u001b[0;32m-> 1722\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mdoc_controls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_not_doc_inheritable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/legacy_tf_layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m       \u001b[0;31m# Actually call layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    764\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 766\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    767\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2104\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2105\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2106\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2107\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2108\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1183\u001b[0m                        'should be defined. Found `None`.')\n\u001b[1;32m   1184\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_ndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlast_dim\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1185\u001b[0;31m     self.kernel = self.add_weight(\n\u001b[0m\u001b[1;32m   1186\u001b[0m         \u001b[0;34m'kernel'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlast_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/legacy_tf_layers/base.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, use_resource, synchronization, aggregation, partitioner, **kwargs)\u001b[0m\n\u001b[1;32m    450\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minitializer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m           \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscope\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m         variable = super(Layer, self).add_weight(\n\u001b[0m\u001b[1;32m    453\u001b[0m             \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m             \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_v1.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0mcaching_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     variable = self._add_variable_with_custom_getter(\n\u001b[0m\u001b[1;32m    441\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    803\u001b[0m         \u001b[0;31m# \"best effort\" to set the initializer with the highest restore UID.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m         \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheckpoint_initializer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m     new_variable = getter(\n\u001b[0m\u001b[1;32m    806\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m         \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1575\u001b[0m                  \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVariableSynchronization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAUTO\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1576\u001b[0m                  aggregation=VariableAggregation.NONE):\n\u001b[0;32m-> 1577\u001b[0;31m   return get_variable_scope().get_variable(\n\u001b[0m\u001b[1;32m   1578\u001b[0m       \u001b[0m_get_default_variable_store\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1579\u001b[0m       \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m       return var_store.get_variable(\n\u001b[0m\u001b[1;32m   1321\u001b[0m           \u001b[0mfull_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    574\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 576\u001b[0;31m       return _true_getter(\n\u001b[0m\u001b[1;32m    577\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    527\u001b[0m             \"name was already created with partitioning?\" % name)\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 529\u001b[0;31m       return self._get_single_variable(\n\u001b[0m\u001b[1;32m    530\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint, synchronization, aggregation)\u001b[0m\n\u001b[1;32m    890\u001b[0m         \u001b[0;31m# default case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m         \u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtb\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m\"tensorflow/python\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 892\u001b[0;31m         raise ValueError(\"%s Originally defined at:\\n\\n%s\" %\n\u001b[0m\u001b[1;32m    893\u001b[0m                          (err_msg, \"\".join(traceback.format_list(tb))))\n\u001b[1;32m    894\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Variable f_network/dense/kernel already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"../../spear/Implyloss/my_networks.py\", line 88, in f_network_fully_connected\n    cur_layer = tf.layers.dense(prev_layer, num_neurons,\n  File \"../../spear/Implyloss/my_model.py\", line 172, in make_f_d_train_ops\n    self.f_d_logits = self.f_network(f_dict, self.num_classes,\n  File \"../../spear/Implyloss/my_model.py\", line 100, in __init__\n    self.make_f_d_train_ops()\n  File \"<ipython-input-6-7277876691ff>\", line 122, in <module>\n    hls = HighLevelSupervisionNetwork(\n  File \"/home/parth/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3441, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# from my_data_feeders import DataFeeder\n",
    "\n",
    "# from my_model import HighLevelSupervisionNetwork\n",
    "# import my_networks\n",
    "from spear.Implyloss import *\n",
    "import numpy as np\n",
    "import sys, os, shutil\n",
    "\n",
    "checkpoint_dir =  './checkpoint'\n",
    "# data_dir = \"/home/parth/Desktop/SEM6/RnD/Learning-From-Rules/data/TREC\" # Directory containing data pickles\n",
    "# data_dir = \"/home/parth/Desktop/SEM6/RnD/spear/examples/SMS_SPAM/data_pipeline/\"\n",
    "data_dir = \"../../examples/SMS_SPAM/data_pipeline/\"\n",
    "inference_output_dir = './inference_output/'\n",
    "log_dir = './logs'\n",
    "metric_pickle_dir = './met_pickl/'\n",
    "tensorboard_dir =  './tensorboard'\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(inference_output_dir):\n",
    "    os.makedirs(inference_output_dir)\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(metric_pickle_dir):\n",
    "    os.makedirs(metric_pickle_dir)\n",
    "\n",
    "if not os.path.exists(tensorboard_dir):\n",
    "    os.makedirs(tensorboard_dir)\n",
    "\n",
    "checkpoint_load_mode = 'mru' # Which kind of checkpoint to restore from. Possible options are mru: Most recently saved checkpoint. Use this to continue a run f_d, f_d_U: Use these to load the best checkpoint from these runs \n",
    "# d_pickle = data_dir+\"d_processed.p\"\n",
    "d_pickle = data_dir+\"sms_pickle_L.pkl\"\n",
    "dropout_keep_prob =  0.8\n",
    "early_stopping_p = 20 # early stopping patience (in epochs)\n",
    "f_d_adam_lr =  0.0003 # default = 0.01\n",
    "f_d_batch_size = 16\n",
    "f_d_class_sampling = [10,10] # Comma-separated list of number of times each d instance should be sampled depending on its class for training f on d. Size of list must equal number of classes.\n",
    "f_d_epochs = 4 # default = 2\n",
    "f_d_metrics_pickle = metric_pickle_dir+\"metrics_train_f_on_d.p\"\n",
    "f_d_primary_metric = 'accuracy' #'f1_score_1' # Metric for best checkpoint computation. The best metrics pickle will also be stored on this basis. Valid values are: accuracy: overall accuracy. f1_score_1: f1_score of class 1. avg_f1_score: average of all classes f1_score \n",
    "f_d_U_adam_lr =  0.0003 # default = 0.01\n",
    "f_d_U_batch_size = 32\n",
    "f_d_U_epochs = 4 # default = 2  \n",
    "f_d_U_metrics_pickle = metric_pickle_dir+\"metrics_train_f_on_d_U.p\"\n",
    "f_infer_out_pickle = inference_output_dir+\"infer_f.p\" # output file name for any inference that was ran on f (classification) network\n",
    "gamma = 0.1 # weighting factor for loss on U used in implication, pr_loss, snorkel, generalized cross entropy etc. \n",
    "lamda = 0.1\n",
    "min_rule_coverage = 0 # Minimum coverage of a rule in U in order to include it in co-training. Rules which have coverage less than this are assigned a constant weight of 1.0.\n",
    "mode = \"learn2reweight\" # \"learn2reweight\" / \"implication\" / \"pr_loss\" / \"label_snorkel\" / \"gcross\" / \"gcross_snorkel\" / \"f_d\" \n",
    "test_mode = \"\" # \"\" / test_f\" / \"test_w\" / \"test_all\"\n",
    "num_classes = 2 # can be 0. Number of classes. If 0, this will be dynamically determined using max of labels in 'd'.\n",
    "num_load_d = None # can be 0. Number of instances to load from d. If 0 load all.\n",
    "num_load_U = None # can be 0. Number of instances to load from U. If 0 load all.\n",
    "num_load_validation = None # can be 0. Number of instances to load from validation. If 0 load all.\n",
    "q = \"1\"\n",
    "rule_classes = None # Comma-separated list of the classes predicted by each rule if string is empty, rule classes are determined from data associated with rule firings.\n",
    "shuffle_batches = True # Don't shuffle batches. Useful for debugging and stepping through batch by batch\n",
    "test_w_batch_size = 1000\n",
    "# U_pickle = data_dir+\"U_processed.p\"\n",
    "U_pickle = data_dir+\"sms_pickle_U.pkl\"\n",
    "use_joint_f_w = False # whether to utilize w network during inference\n",
    "# validation_pickle = data_dir+\"validation_processed.p\"\n",
    "validation_pickle = data_dir+\"sms_pickle_V.pkl\"\n",
    "w_infer_out_pickle = inference_output_dir+\"infer_w.p\" # output file name for any inference that was ran on w (rule) network\n",
    "json_file = data_dir+\"sms_json.json\"\n",
    "\n",
    "output_dir = \"./\" + str(mode) + \"_\" + str(gamma) + \"_\" + str(lamda) + \"_\" + str(q)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "if test_mode==\"\":\n",
    "    if os.path.exists(checkpoint_dir):\n",
    "        shutil.rmtree(checkpoint_dir, ignore_errors=True)    \n",
    "    os.makedirs(checkpoint_dir)\n",
    "\n",
    "# number of input dir - 1 (data_dir)\n",
    "# number of output dir - 6 (checkpoint, inference_output, log_dir, metric_pickle, output, tensorboard)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if(str(test_mode)==\"\"):\n",
    "        output_text_file=log_dir + \"/\" + str(mode) + \"_\" + str(gamma) + \"_\" + str(lamda) + \"_\" + str(q)+\".txt\"\n",
    "    else:    \n",
    "        output_text_file=log_dir + \"/\" + str(test_mode) + \"_\" + str(mode) + \"_\" + str(gamma) + \"_\" + str(lamda) + \"_\" + str(q)+\".txt\"\n",
    "    sys.stdout = open(output_text_file,\"w\")\n",
    "    if(test_mode!=\"\"):\n",
    "        mode = test_mode\n",
    "    if mode not in ['learn2reweight', 'implication', 'f_d', 'pr_loss', 'gcross',  'label_snorkel', 'pure_snorkel', 'gcross_snorkel', 'test_f', 'test_w', 'test_all']:\n",
    "        raise ValueError('Invalid run mode ' + mode)\n",
    "\n",
    "    data_feeder = DataFeeder(d_pickle, \n",
    "                             U_pickle, \n",
    "                             validation_pickle,\n",
    "                             json_file,\n",
    "                             shuffle_batches, \n",
    "                             num_load_d, \n",
    "                             num_load_U, \n",
    "                             num_classes, \n",
    "                             f_d_class_sampling, \n",
    "                             min_rule_coverage, \n",
    "                             rule_classes, \n",
    "                             num_load_validation, \n",
    "                             f_d_batch_size, \n",
    "                             f_d_U_batch_size, \n",
    "                             test_w_batch_size,\n",
    "                             out_dir=output_dir)\n",
    "\n",
    "    num_features, num_classes, num_rules, num_rules_to_train = data_feeder.get_features_classes_rules()\n",
    "    print(\"Number of features: \", num_features)\n",
    "    print(\"Number of classes: \",num_classes)\n",
    "    print(\"Print num of rules to train: \", num_rules_to_train)\n",
    "    print(\"Print num of rules: \", num_rules)\n",
    "    print(\"\\n\\n\")\n",
    "    rule_classes = data_feeder.rule_classes\n",
    "    w_network = my_networks.w_network_fully_connected #rule network - CHANGE config in w_network_fully_connected of my_networks - DONE\n",
    "    f_network = my_networks.f_network_fully_connected #classification network - CHANGE config in f_network_fully_connected of my_networks - DONE\n",
    "    hls = HighLevelSupervisionNetwork(\n",
    "            num_features,\n",
    "            num_classes,\n",
    "            num_rules,\n",
    "            num_rules_to_train,\n",
    "            rule_classes,\n",
    "            w_network,\n",
    "            f_network,\n",
    "            f_d_epochs, \n",
    "            f_d_U_epochs, \n",
    "            f_d_adam_lr, \n",
    "            f_d_U_adam_lr, \n",
    "            dropout_keep_prob, \n",
    "            f_d_metrics_pickle, \n",
    "            f_d_U_metrics_pickle, \n",
    "            early_stopping_p, \n",
    "            f_d_primary_metric, \n",
    "            mode, \n",
    "            data_dir, \n",
    "            tensorboard_dir, \n",
    "            checkpoint_dir, \n",
    "            checkpoint_load_mode, \n",
    "            gamma, \n",
    "            lamda,\n",
    "            raw_d_x=data_feeder.raw_d.x, #instances from the \"d\" set\n",
    "            raw_d_L=data_feeder.raw_d.L) #labels from the \"d\" set\n",
    "\n",
    "    # Output 3 digits after decimal point in numpy arrays\n",
    "    float_formatter = lambda x: \"%.3f\" % x\n",
    "    np.set_printoptions(formatter={'float_kind':float_formatter})\n",
    "\n",
    "    print('Run mode is ' + mode)\n",
    "    if mode == 'f_d':\n",
    "        print('training f on d')\n",
    "        hls.train.train_f_on_d(data_feeder, f_d_epochs)\n",
    "    elif mode == 'implication':\n",
    "        print(\"begin Implication loss training\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='implication')\n",
    "        print(\" Implication loss training end\")\n",
    "    elif mode == 'pr_loss':\n",
    "        print(\"begin pr_loss training\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='pr_loss')\n",
    "        print(\"pr_loss training end\")\n",
    "    elif mode == 'gcross': # majority_label\n",
    "        print(\"gcross\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='gcross')\n",
    "    elif mode == 'gcross_snorkel':\n",
    "        print(\"gcross_snorkel\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='gcross_snorkel')\n",
    "    elif mode == 'learn2reweight':\n",
    "        print('learn2reweight')\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='learn2reweight')\n",
    "    elif mode == 'pure_snorkel':\n",
    "        print(\"pure_snorkel\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='pure_snorkel')\n",
    "    elif mode == 'label_snorkel':\n",
    "        print(\"label_snorkel\")\n",
    "        hls.train.train_f_on_d_U(data_feeder, f_d_U_epochs, loss_type='label_snorkel')\n",
    "    elif mode == 'test_f':\n",
    "        print('Running test_f')\n",
    "        hls.test.test_f(data_feeder, log_output=True, \n",
    "                        save_filename=f_infer_out_pickle, \n",
    "                        use_joint_f_w=use_joint_f_w)\n",
    "    elif mode == 'test_w':\n",
    "        print('Running test_w')\n",
    "        hls.test.test_w(data_feeder, log_output=True, save_filename=w_infer_out_pickle+\"_test\")\n",
    "    elif mode == 'test_all':\n",
    "        print('Running all tests')\n",
    "        print('\\ninference on f network ...\\n')\n",
    "        hls.test.test_f(data_feeder, log_output=True, \n",
    "                        save_filename=f_infer_out_pickle,\n",
    "                        use_joint_f_w=use_joint_f_w)\n",
    "        print('\\ninference on w network...')\n",
    "        print('we only test on instances covered by atleast one rule\\n')\n",
    "        hls.test.test_w(data_feeder, log_output=True, save_filename=w_infer_out_pickle+\"_test\")\n",
    "    else:\n",
    "        assert not \"Invalid mode string: %s\" % mode\n",
    "\n",
    "    sys.stdout.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
